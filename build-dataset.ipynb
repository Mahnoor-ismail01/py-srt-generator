{"cells":[{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDQDpoaaXMxl","executionInfo":{"status":"ok","timestamp":1643882015207,"user_tz":-300,"elapsed":3064,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}},"outputId":"e56be960-b36f-488c-f037-2ef58e397cf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"CXxbuJO_WT0K"},"source":["# Generate dataset"]},{"cell_type":"code","source":["!pip install decoder"],"metadata":{"id":"0RD8IpDj0tGp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643882017878,"user_tz":-300,"elapsed":2683,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}},"outputId":"e00e5e78-273e-4866-8bd0-4412e1645233"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: decoder in /usr/local/lib/python3.7/dist-packages (0.5)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from decoder) (1.1.0)\n","Requirement already satisfied: pycipher in /usr/local/lib/python3.7/dist-packages (from decoder) (0.5.2)\n"]}]},{"cell_type":"code","execution_count":53,"metadata":{"id":"oh0ovTTSWT0Q","executionInfo":{"status":"ok","timestamp":1643882017879,"user_tz":-300,"elapsed":23,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"outputs":[],"source":["#from csrc.build import generate_from_audio\n","#from csrc.configurations import DatasetConfig as DC\n","#from utils import check_type, extract_audio, count_class\n","import decoder"]},{"cell_type":"markdown","metadata":{"id":"zaTalhevW1L2"},"source":["# CSRD CONFIG"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"cAkPbZc9W3iJ","executionInfo":{"status":"ok","timestamp":1643882017881,"user_tz":-300,"elapsed":24,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"outputs":[],"source":["\"\"\"\n","Those are the configuration settings.\n","\"\"\"\n","\n","class ModelConfig(object):\n","    # Configurations for SED model.\n","    sed_model_config = {\n","        \"sample_rate\": 32000,\n","        \"window_size\": 1024,\n","        \"hop_size\": 320,\n","        \"mel_bins\": 64,\n","        \"fmin\": 50,\n","        \"fmax\": 14000,\n","        \"classes_num\": 527,\n","    }\n","    \n","class DatasetConfig(object):\n","    \"\"\"\n","    Configuration for building your own dataset from sources.\n","    \n","    Attributes:\n","        dataset_clip_time(int): Clip length for dataset. Default 2s.\n","        dataset_sample_rate(int): Clip length for dataset. Default 32000.\n","        dataset_audio_format(str): Clip format for dataset. Default using \"wav\"\n","        sub_encoding(str): Use \"utf-8\" encoding for Aegisub subtitle support.\n","    \"\"\"\n","    \n","    dataset_clip_time = 2 # seconds\n","    \n","    dataset_sample_rate = 32000\n","    \n","    dataset_audio_format = \"wav\" \n","    \n","    sub_encoding = \"utf-8\" # recommanded\n","    "]},{"cell_type":"markdown","source":["#UTILS.py Main"],"metadata":{"id":"wL_d02Ka7A27"}},{"cell_type":"code","source":["\"\"\"\n","Preprocess some input:\n","\n","- If it\"s video, extract audio from it.\n","- If it\"s audio, do nothing.\n","\n","To run the audio extraction process normally, you should name the video right\n","in the same standard as the audio and make sure it supports the format the \n","Python Module Moviepy supports.\n","\n","Check \"https://pypi.org/project/moviepy/\" for formats Moviepy supports.\n","\"\"\"\n","\n","from pathlib import Path\n","import os\n","import time\n","import mimetypes\n","import os\n","\n","import librosa\n","import moviepy.editor as mp\n","\n","def mono_load(path, sr=32000, mono=True):\n","    \"\"\"\n","    A custonmized librosa loading process emphasizing mono channel.\n","    \n","    Args:\n","        path: Audio file path.\n","        sr: Sample rate for librosa loading.\n","        mono: Indicate mono channel for the output.\n","        \n","    Returns: \n","        y: Librosa loading output.\n","        c: The number of channels for the original audio file. \n","    \"\"\"\n","    \n","    start = time.time()\n","    \n","    print(f\"Loading file: {path}\")\n","    y, c = librosa.load(path, sr=sr, mono=mono)\n","    \n","    end = time.time()\n","    \n","    print(f\"Loading completed. Cost {(end-start):.2f}s\\n\")\n","    \n","    return y, c \n","    \n","def vb(pre, any, v):\n","    \"\"\"\n","    Verbose print.\n","    \"\"\"\n","    if v:\n","        print(f\"{pre} {repr(any)}\")\n","        \n","def check_type(file_path):\n","    \"\"\"\n","    Check whether it\"s a audio or video straitforwardly.\n","    \n","    Args:\n","        file_path: The path of the file to check type.\n","        \n","    Returns:\n","        is_video: Whether the file is a video or an audio.\n","    \"\"\"\n","    \n","    assert (Path(file_path).exists()) and (Path(file_path).is_file()), \"Your input file path is not valid or the file doesn't exist.\"\n","    \n","    is_video = False\n","    \n","    mimetypes.init()\n","    \n","    mimestart = mimetypes.guess_type(str(file_path))[0]\n","    \n","    if mimestart: # If the metadata can't be pared, it's mostly because the file is an audio file.\n","        try:\n","            mimestart = mimestart.split(\"/\")[0]\n","        except RuntimeError as e:\n","            print(e)\n","            print(\"Unrecognizable file type. Is the file format valid? (Using mimetypes)\\n\")    \n","        \n","        assert mimestart==\"video\" or mimestart==\"audio\", \"Input file format unrecognizable as video or audio (using mimetypes).\\n\"\n","        \n","        if mimestart == \"video\": is_video = True \n","\n","    return is_video\n","\n","def extract_audio(file_path, format: str=\"wav\"):\n","    \"\"\"\n","    Extract audio from video.\n","    \n","    Args:\n","        file_path: File path for the video clip.    \n","        \n","    Returns:\n","        mv_audio_file: Audio file path extracted.\n","    \"\"\"\n","    \n","    print(f\"Extracting audio from {file_path}\")\n","    \n","    mv = mp.VideoFileClip(file_path)\n","    assert mv!=None, \"Unable to extract any information from the video clip.\"\n","    \n","    mv_name = str(file_path).split(\"/\")[-1].split(\".\")[0]\n","    mv_audio_file = Path(file_path).parent / f\"{mv_name}.{format}\"\n","    \n","    # A potential error for moviepy to resolve system Path could trigger AttributeError.\n","    # We catch the error and then use string for movie py to resolve the file path.\n","    # This error occurs on Windows.\n","    try:\n","        mv.audio.write_audiofile(mv_audio_file)\n","    except AttributeError:\n","        print(\"\\nNote: Moviepy failed to resolve your video path. Currently Use Path as string for moviepy to work.\\n\")\n","        mv.audio.write_audiofile(str(mv_audio_file))\n","     \n","    print(f\"Extraction Successful! Writing {Path(mv_audio_file).stat().st_size} in {mv_audio_file}.\")\n","    \n","    return mv_audio_file\n","\n","def count_class(path):\n","    \"\"\"\n","    Count how many instances of 0 and 1 under the folder path.\n","    \n","    Args:\n","        file_path: The path of the folder containing the class instances.\n","        \n","    Returns:\n","        zeros: The number of instances in class 0 (No human speaking).\n","        ones: The numger of instances in class 0 (Human speaking).\n","        total: total instances.\n","    \"\"\"\n","    \n","    zeros = 0\n","    ones = 0\n","    total = 0\n","    \n","    for file in os.listdir(path):\n","        c = str(file).split(\".\")[0][-1]\n","        ones += int(c)\n","        total += 1\n","    zeros = total - ones\n","    \n","    print(f\"\\nLabel 1 instances: {ones}\")\n","    print(f\"Label 0 instances: {zeros}\")\n","    print(f\"Total clips: {total}\\n\")\n","    \n","    return zeros, ones, total\n","\n","def get_duration(audio_file_path, y=None, sr=None):\n","    \"\"\"\n","    Check the consistency between audio header metadata and audio waveform.\n","    \n","    Args:\n","        audio_file_path: The audio file path.\n","        y: Audio time series.\n","        sr: Audio sample rate.\n","        \n","    Returns (one of):\n","        waveform_duration: Audio duration according to the audio waveform.\n","        header_duration: Audio duration accrording to audio metadata.\n","    \"\"\"\n","    \n","    header_duration = librosa.get_duration(filename=audio_file_path)\n","    \n","    if sr:\n","        wavform_duration = librosa.get_duration(y=y, sr=sr)\n","        \n","        if header_duration == wavform_duration:\n","            print(\"Audio file consistency ensured.\")\n","        else:\n","            print(\"There is inconsistency between the audio waveform and header metadata.\" \\\n","                \"This could be ignored.\")\n","            \n","        return wavform_duration\n","    \n","    return header_duration\n","    \n"],"metadata":{"id":"KFgWVhOu7DwN","executionInfo":{"status":"ok","timestamp":1643882018655,"user_tz":-300,"elapsed":23,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FAiwqRxW-QT"},"source":["# CSRC UITLS"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"EixjKIQ4XAH5","executionInfo":{"status":"ok","timestamp":1643882018656,"user_tz":-300,"elapsed":18,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"outputs":[],"source":["\"\"\"\n","Util functions for deep learning.\n","\"\"\"\n","\n","from pathlib import Path\n","import random\n","\n","import numpy as np\n","import torch\n","import os\n","\n","#from config import ROOT_PATH_ABS\n","\n","def seed_all(s:int=42) -> None: \n","    random.seed(s)\n","    np.random.seed(s)\n","    os.environ[\"PYTHONHASHSEED\"] = str(s)\n","    torch.manual_seed(s)\n","    torch.cuda.manual_seed(s)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","\n","def seed_dataset(s:int=42) -> None:\n","    random.seed(s)\n","    \n","\n","class TrainingDirs(object):\n","    \"\"\"\n","    Initiate the working directory systems for training.\n","    \"\"\"\n","    \n","    def __init__(self, dsname: str, pre_test: bool) -> None:\n","        super().__init__()\n","        ROOT = Path(ROOT_PATH_ABS)\n","        INPUT_ROOT = ROOT / 'data'\n","        TARGET_AUDIO_DIR = INPUT_ROOT / dsname\n","        print(f\"Working with dataset under {TARGET_AUDIO_DIR}.\")\n","        assert os.path.exists(TARGET_AUDIO_DIR), \"Input dataset folder does not exist.\"\n","        \n","        self.dataset_folder = TARGET_AUDIO_DIR\n","        self.train_folder = TARGET_AUDIO_DIR / 'train' if pre_test else TARGET_AUDIO_DIR\n","        self.test_folder = TARGET_AUDIO_DIR / 'test' if pre_test else None\n","    "]},{"cell_type":"markdown","metadata":{"id":"cQ8dD0RkWkuQ"},"source":["# CSRC BUILD"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"JvRaLYwOWo_8","executionInfo":{"status":"ok","timestamp":1643882018657,"user_tz":-300,"elapsed":15,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"outputs":[],"source":["\"\"\"Generate audio clips we need for training.\n","\n","1. Generate 2secs clips for training, wav file format with unique id.\n","2. Generate training csv file corresponding to the whole dataset\n","\"\"\"\n","\n","from pathlib import Path\n","\n","import soundfile as sf\n","import numpy as np\n","\n","#from csrc.configurations import DatasetConfig\n","#from utils import mono_load, vb\n","\n","def _tagging(clip_ending_second, onsets, offsets):\n","    \"\"\"\n","    Tag the audio by judging whether the clip contains a dialogue.\n","    If the clip contains dialogues in it, we tag it as 1. Otherwise we tag it as 0.\n","    \"\"\"\n","    assert int(clip_ending_second)%DatasetConfig.dataset_clip_time == 0., 'Sorry, there is a length mismatch when trying to tagging the clip.'\n","    \n","    tag = 0\n","    clip_beginning_second = clip_ending_second - 5\n","    for _, dialogue in enumerate(zip(onsets, offsets)):\n","        \n","        if dialogue[0] > clip_ending_second: break\n","        \n","        if clip_beginning_second < dialogue[0] and clip_ending_second > dialogue[0]:\n","            tag = 1\n","            break\n","        \n","        if clip_beginning_second > dialogue[1] and clip_ending_second < dialogue[1]:\n","            tag = 1 \n","            break\n","        \n","    return tag\n","\n","def _resample(y, name, clip_format, index, path, verbose=False):\n","    output_path = f'{path}/{index}-{name}.{clip_format}'\n","    if verbose:\n","        vb(\"Making file:\", output_path, verbose)\n","    sf.write(output_path, y, DatasetConfig.dataset_sample_rate, format=clip_format, subtype='PCM_24')\n","\n","def generate_from_audio(audio_path, sub_path, dest_path, sub_decoder, verbose=False):\n","    \"\"\"Generate a new dataset from video.\n","    \n","    Args:\n","        audio_path: The file path of the audio.\n","        sub_path: The file path of the subtitle.\n","        dest_path: The destination path of the well-formatted dataset.\n","        sub_decoder: The decoder to get sub file format and events.\n","    \"\"\"\n","        \n","    # Get name suffix.\n","    video_name = audio_path.stem if isinstance(audio_path, Path) else Path(audio_path).stem\n","    \n","    # Get formatted sub events for further engineering.\n","    print(\"Extracting timestamps from the subtitle file...\")\n","    decoder = sub_decoder(sub_path, encoding=DatasetConfig.sub_encoding)\n","    onsets, offsets = decoder.time_series\n","    if verbose:\n","        vb('Onset timestamps generated:', onsets, verbose)\n","        vb('Offset timestamps generated:', offsets, verbose)\n","        \n","    print(\"Extraction complete!\\n\")\n","\n","    # Load audio using librosa and resample the audio in this step.\n","    print(\"Librosa loading audio...\")\n","    y, sr = mono_load(audio_path)\n","    print(f\"Loading source file success! Using sampling rate {DatasetConfig.dataset_sample_rate}.\\n\")\n","    \n","    # Get the clip sample lengths.\n","    clip_sample_length = sr * DatasetConfig.dataset_clip_time\n","    \n","    # Main loop.\n","    # Divide a whole audio into clips.\n","    clip_flag = 0 # current working clip in current sound position (not senconds)\n","    idx = 0 # current working clip number \n","    exceeded = False # whether the clip has exceeded the audio file\n","    sound = y\n","    tag_1 = 0\n","    tag_0 = 0\n","\n","    print(\"Start building process.\\nTransforming dataset...\\n\")\n","    while True:\n","        if clip_flag + clip_sample_length > len(sound):\n","            padding = clip_flag + clip_sample_length - len(sound)\n","            clip = np.zeros(clip_sample_length)\n","            clip[:-padding] = sound[clip_flag:]\n","            exceeded = True\n","        else:\n","            clip = sound[clip_flag: clip_flag+clip_sample_length]\n","            \n","        clip_flag += clip_sample_length\n","        idx +=1\n","         \n","        # Get the clip name with corresponding labels with timeline of this clip and sub_events.\n","        clip_tag = _tagging(clip_flag/sr, onsets, offsets)\n","        if clip_tag == 0:\n","            tag_0 += 1\n","        if clip_tag == 1:\n","            tag_1 += 1\n","            \n","        clip_name = video_name + '-' + str(clip_tag)\n","        _resample(clip, clip_name, DatasetConfig.dataset_audio_format, idx, dest_path, verbose)\n","        \n","        # If exceeded, break the loop.\n","        if exceeded: break\n","    \n","    print(f\"Building process finished for {video_name}.\")\n","    print(f\"Label 1 (speech) clips: {tag_1}\\nLabel 0 (non-speech) clips: {tag_0} \\n\")\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"pJjnYpmAWT0S"},"source":["## user configurations"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"EZoZH3HrWT0T","executionInfo":{"status":"ok","timestamp":1643887222591,"user_tz":-300,"elapsed":11,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"outputs":[],"source":["# Kowning: the name of the audio is restrained to (name-lang).xxx, you can define the name as you like but remember to add the  standard language type as lang.\n","# You can name your subtitle file as your want.\"\n","#test_clips = (\"./src/src-test/test-eng.mp4\", \"./src/src-test/test.ass\")\n","\n","dbc = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/data movie/1.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/movie srt/1.srt\")\n","ab = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/data movie/2.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/movie srt/2.srt\")\n","lstsb = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/data movie/3.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/movie srt/3.srt\")\n","mi = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/data movie/4.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/movie srt/4.srt\")\n","tdkr = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/data movie/5.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/movie srt/5.srt\")\n","tkoh = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/data movie/6.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/movie srt/6.srt\")\n","tks = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/data movie/7.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/movie srt/7.srt\")\n","lms = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v1/1.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v1/1.srt\")\n","asf = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v2/2.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v2/2.srt\")\n","frt = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v3/3.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v3/3.srt\")\n","ewe = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v4/4.mp4\", \"/content/drive/MyDrive/Auto-Subtitle-File-Generation/src/clips/v4/4.srt\")\n","\n","dataset_source_long = [dbc, ab, lstsb, mi, tdkr, tkoh, tks, lms, asf, frt, ewe]\n","\n","#dataset_source_short = [dbc, ab, lstsb]\n","\n","#dataset_source_medium = [dbc, ab, lstsb, mi, tdkr]\n","\n","sources = [\n","    # (audio file path under src, subtitle file path correspondingly under src)\\n\",\n","    *dataset_source_long\n","]\n","\n","# Destination folder for you to store current sources clips.\n","# It\"s ok if you want to store clips directly under the data folder.\n","# and this won\"t effect whether you want to separate the train/test folder by yourself.\n","\n","dest_path = (\"/content/drive/MyDrive/Auto-Subtitle-File-Generation/data/standard-p2-32khz\")"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"tcwEY88AWT0V","outputId":"1a6112fe-c352-48c3-ef45-d3df0916fe85","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643887226683,"user_tz":-300,"elapsed":366,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Label 1 instances: 189\n","Label 0 instances: 234\n","Total clips: 423\n","\n"]}],"source":["_, _, _ = count_class(dest_path)"]},{"cell_type":"markdown","source":["#Decoder"],"metadata":{"id":"1WJe3suD7len"}},{"cell_type":"code","source":["\"\"\"Decode subtitle files for materials bulding the dataset.\n","\"\"\"\n","\n","from pathlib import Path\n","#from config import DecoderConfig as DC\n","\n","\n","class Decoder(object):\n","    \"\"\"Decoder containing public arguments and methods for subtitle files.\n","    \"\"\"\n","    \n","    def __init__(self, file_path, encoding, trim) -> None:\n","        super().__init__()\n","        self.file_path = file_path\n","        self.encoding = encoding\n","        self.trim = trim\n","        \n","    def _trim_events(self, onsets, offsets):\n","        \"\"\"Trim the events.\n","\n","        Sometimes the subtitle will continue after cutting out a clip (2s) but that time \n","        is very short (like 0.1s) that nobody (let alone machines) can recognize human \n","        speech in it. To avoid such bias causing a lot of mislabeled, unrecognizable clips\n","        in the dataset. we should trim the event onsets and offsets to keep the obvious, \n","        recognizable speech clips only.\n","        \n","        Args:\n","            onsets: [events onsets].\n","            offsets: [events offsets].\n","            \n","        Returns: \n","            onsets: [trimmed events onsets].\n","            offsets: [trimmed events offsets].\n","        \"\"\"\n","        \n","        def handle_offset(offset):\n","            if (offset % 2.) < DatasetConfig.trimming_end:\n","                offset = float(int(offset))\n","            \n","            return offset\n","                \n","        \n","            \n","        # Trim offsets.\n","        offsets = list(map(handle_offset, offsets))\n","        \n","        # Trim onsets.\n","        onsets = list(map(handle_onset, onsets))\n","        \n","        return onsets, offsets\n","\n","\n","class ASSDecoder(Decoder):\n","    \"\"\"\n","    Decode .ass(.ssa) subtitle files\n","    \n","    Args: \n","        file_path: The file path of the subtitle file.\n","        encoding: The encoding of the subtitle file.\n","        \n","    Attributes:\n","        file_type: Subtitle file format.\n","        \n","    Properties:\n","        time_series: Containing all events timestamps (s).\n","    \"\"\"\n","    \n","    file_type = \"ass\"\n","\n","    def __init__(self, file_path, encoding=\"utf-8\", trim=True):\n","        assert isinstance(file_path, str) or isinstance(file_path, Path), \"Invalid file path, only 'str' and Pathlib.Path' supported.\"\n","        super().__init__(file_path, encoding, trim)\n","        self.flag = 0\n","        self.tags =  self._tags()\n","        assert len(self.tags[\"events\"])==1, \"There should only be one [Events] tag in sub file.\"\n","        assert len(self.tags.keys()) == 3, \"Your sub file should only and must contain following components: headers lines, [...Styles], [Events].\"\n","\n","    def _tags(self):\n","\n","        # iterate the whole file and return tags of headers/styles/events\n","        with open(self.file_path, encoding=self.encoding) as f:\n","            tags = dict()\n","            tags[\"headers\"] = 0\n","            tags[\"events\"] = list()\n","            tags[\"styles\"] = list()\n","            for i, line in enumerate(f.readlines()):\n","                if \"events\" in line.lower() and line.startswith(\"[\"):\n","                    tags[\"events\"].append(i)\n","                if \"styles\" in line.lower() and line.startswith(\"[\"):\n","                    tags[\"styles\"].append(i)\n","\n","        return tags\n","    \n","    def _decode_time(self, str_time):\n","        \"\"\"Decode time from src to float(.2f), which stands for seconds.\n","        \n","        Returns:\n","            float_time: Seconds of the corresponding time.\n","            \n","        Args:\n","            str_time: String format for time object.\n","            \n","        Properties:\n","            events: Parsed events output.\n","            time_series: series of begining and ending timestamps.\n","        \"\"\"\n","        \n","        tail = float(str_time.split(\".\")[-1]) * 1e-2\n","        h, m, s = str_time.split(\".\")[0].split(\":\")\n","        float_time = int(h)*3600 + int(m)*60 + int(s) + tail\n","        \n","        return float_time\n","\n","    @property\n","    def events(self):\n","        with open(self.file_path, encoding=self.encoding) as f:\n","            assert len(self.tags[\"events\"])==1, \"There should only be one [Events] tag in sub file.\"\n","\n","            events = []\n","            events_info = {\n","                \"tag\": \"\",\n","                \"header\": \"\",\n","                \"features\": \"\"\n","            }\n","\n","            # get well formatted sub events list\n","            events = f.readlines()[self.tags[\"events\"][0]:]\n","            events = [event.lstrip().rstrip() for event in events if event != \"\\n\"]\n","\n","            # collect events information\n","            events_tag = events[0]\n","            events_header = events[1]\n","            features = events_header.split(\":\")[1].split(\",\")\n","            assert len(features) == 10, \"Events feature number does not fit the standard, please check your sub file.\"\n","\n","            events_info[\"tag\"] = events_tag\n","            events_info[\"header\"] = events_header\n","            events_info[\"features\"] = features\n","\n","        return events[2:], events_info\n","    \n","    @property\n","    def time_series(self):\n","        \"\"\"Return two timestamp lists, in a list each element stands for the beginning or the end of a dialogue.\n","\n","        Returns:\n","            on_ts: List of all event onsets.\n","            off_ts: List of all event offsets.\n","        \"\"\"\n","        \n","        events, _ = self.events\n","        on_ts = list()\n","        off_ts = list()\n","        \n","        assert events is not None and events is not [], \"Events empty, can not generate time series\"\n","            \n","        for event in events:\n","            # remove duplicated timeseries, for multilanguage sub file we directly ingore them\n","            # we assume all languages\" sub file are in the same time series\n","            \n","            onset = event.split(\",\")[1]\n","            offset = event.split(\",\")[2]\n","            \n","            if onset in on_ts or offset in off_ts:\n","                break\n","\n","            on_ts.append(self._decode_time(onset))\n","            off_ts.append(self._decode_time(offset))\n","            \n","        on_ts, off_ts = self._trim_events(on_ts, off_ts) if self.trim else (on_ts, off_ts)\n","            \n","        assert len(on_ts) == len(off_ts), \"Unable to match onset with offset for Dialogues, please check your sub file\"\n","        \n","        return on_ts, off_ts\n","             \n","            \n","class SRTDecoder(Decoder):\n","    \"\"\"Decode .srt format subtitle files.\n","    \n","    Args:\n","        file_path(str/path): The subtitle file path for decoding.\n","        encoding(str): The encoding of the subtitle file.\n","    \n","    Attributes:\n","        file_type: The subtitle file type.\n","        \n","    Properties:\n","        time_series -> on_ts, off_ts: Timestamp collections of the beginning and ending of each event. \n","    \"\"\"\n","    \n","    file_type = \"srt\"\n","    \n","    def __init__(self, file_path, encoding=\"utf-8\", trim=True) -> None:\n","        assert isinstance(file_path, str) or isinstance(file_path, Path), \"Invalid file path, only 'str' and Pathlib.Path' supported.\"\n","        super().__init__(file_path, encoding, trim)\n","\n","    def _decode_time(self, str_time):\n","        \"\"\"Decode time from src to float(.2f), which stands for seconds.\n","        \n","        Returns:\n","            float_time: Seconds of the corresponding time.\n","            \n","        Args:\n","            str_time: String format for time object.\n","            \n","        Properties:\n","            events: Parsed events output.\n","            time_series: series of begining and ending timestamps.\n","        \"\"\"\n","                \n","        tail = float(str_time.split(\",\")[-1]) * 1e-3\n","        h, m, s = str_time.split(\",\")[0].split(\":\")\n","        float_time = int(h)*3600 + int(m)*60 + int(s) + tail\n","        \n","        return float_time\n","        \n","    @property\n","    def time_series(self):\n","        \"\"\"Return event timestamps.\n","        \"\"\"\n","        on_ts = []\n","        off_ts = []\n","        \n","        with open(self.file_path, mode=\"r\", encoding=self.encoding) as f:\n","            for line in f.readlines():\n","                if \"-->\" in line:\n","                    onset = line.split(\"-\")[0].lstrip().rstrip()\n","                    offset = line.split(\">\")[-1].lstrip().rstrip()\n","                    onset = self._decode_time(onset)\n","                    offset = self._decode_time(offset)\n","                    if onset:\n","                        on_ts.append(onset)\n","                    if offset: \n","                        off_ts.append(offset)\n","                        \n","        on_ts, off_ts = self._trim_events(on_ts, off_ts) if self.trim else (on_ts, off_ts)\n","        \n","        assert len(on_ts)==len(off_ts), \"Mismatch for timestamp series.\"\n","\n","        return on_ts, off_ts\n"],"metadata":{"id":"XhwXVQtt7nWI","executionInfo":{"status":"ok","timestamp":1643882019231,"user_tz":-300,"elapsed":584,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wIO2DrP0WT0W"},"source":["## Start auto generation"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"EePppLhBWT0Y","executionInfo":{"status":"error","timestamp":1643887258148,"user_tz":-300,"elapsed":469,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}},"colab":{"base_uri":"https://localhost:8080/","height":252},"outputId":"c37c923c-495e-4141-b738-b1ef50814833"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-779fbce844eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msubtitle_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASSDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubtitle_file_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"srt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0msubtitle_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSRTDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mis_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maudio_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_audio_format\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_video\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msource_file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'decoder' has no attribute 'SRTDecoder'"]}],"source":["for source_file_path, subtitle_file_path in sources: \n","    if subtitle_file_path.split(\".\")[-1]==\"ass\":\n","        subtitle_decoder = decoder.ASSDecoder\n","    if subtitle_file_path.split(\".\")[-1]==\"srt\":\n","      subtitle_decoder = decoder.SRTDecoder\n","    is_video = check_type(source_file_path)\n","    audio_file_path = extract_audio(source_file_path, format=DatasetConfig.dataset_audio_format) if is_video else source_file_path\n","    generate_from_audio(audio_path=audio_file_path, sub_path=subtitle_file_path, dest_path=dest_path, sub_decoder=subtitle_decoder,  verbose=True)"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"oEFWSJjxWT0Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643882697778,"user_tz":-300,"elapsed":369,"user":{"displayName":"Mansoor Ahmed","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07436205017683697628"}},"outputId":"09a4864d-5189-4847-85bc-5f0b13653cc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Label 1 instances: 189\n","Label 0 instances: 234\n","Total clips: 423\n","\n"]}],"source":["_, _, _ = count_class(dest_path)"]}],"metadata":{"colab":{"name":"build-dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}